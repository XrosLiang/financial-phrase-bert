{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SA_Tweet_LSTMvsBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "311d0ea8f9db4db9bb17f89182fd4abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89db5b7536df4d82ac976fccb1b11043",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a88a8f837124f3892888d1138e60ff0",
              "IPY_MODEL_fef32952b6d44e8f8de86e69fca18c3d"
            ]
          }
        },
        "89db5b7536df4d82ac976fccb1b11043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a88a8f837124f3892888d1138e60ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5c51e4d4ac5b42e6bce3e71c33159ca4",
            "_dom_classes": [],
            "description": " 67%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1548010,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1048573,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a8da457b7444ba38215c12610f43606"
          }
        },
        "fef32952b6d44e8f8de86e69fca18c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3da80801864456c94ce2a20fd620b15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1037883/1548010 [00:10&lt;00:04, 105439.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ff0d18991074416baa9d2dc225214b8"
          }
        },
        "5c51e4d4ac5b42e6bce3e71c33159ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a8da457b7444ba38215c12610f43606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3da80801864456c94ce2a20fd620b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ff0d18991074416baa9d2dc225214b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B2_4RC83PD9"
      },
      "source": [
        "# Sentiment analysis on tweets\n",
        "\n",
        "The goal of this project is to compare the performance of LSTM and pre-trained BERT under the limited time. When you see performance of an AI model, it often the case that it does not perform as expected for your own problem while it does for the evaluated environment.\n",
        "\n",
        "The major reasons being\n",
        " * time is limited and do not have access to high performance cloud TPUs for optimisation\n",
        " * input data is structurally different\n",
        " * less number of labeled data are available\n",
        "\n",
        "Here, I have 1 million sentiment labeled tweets from Stocktwit. The performance is measured in terms of accuracy and f1 score, spending a small and the same amount of time for hyperparameter tuning.\n",
        "\n",
        "My expectation is\n",
        " * Normal LSTM can perform well on tweet type of text, as it usually does not have long complex sentence structures.\n",
        " * LSTM will overfit when the training samples are not enough but it can be trained when more inputs are avaiable\n",
        " * Pre-trained BERT has been trained Wikipedia+Book Corpus, which is quite different from tweet, thus not performing well while transfer learning is still valid\n",
        "\n",
        "Let's see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHecaO_7JG6L"
      },
      "source": [
        "## Import and configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdwYN7SIw5pd",
        "outputId": "81cfef28-ec77-496a-c76b-b318cb9edf35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Mount Google Drive for Colab\n",
        "\n",
        "import sys\n",
        "\n",
        "# Check Colab Env\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(IN_COLAB)\n",
        "\n",
        "# Enter the oauth code from the link\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-37Bpnkw5pT"
      },
      "source": [
        "# Set logger\n",
        "import logging, time, sys\n",
        "loggers = {}\n",
        "\n",
        "def set_logger(name, level):\n",
        "    global loggers\n",
        "\n",
        "    if loggers.get(name):\n",
        "        return loggers.get(name)\n",
        "    else:\n",
        "        logger = logging.getLogger(name)\n",
        "        if (logger.hasHandlers()):\n",
        "            logger.handlers.clear()\n",
        "            \n",
        "        logger.setLevel(level)\n",
        "\n",
        "        timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
        "        formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
        "\n",
        "        fh = logging.FileHandler(name + '.log')\n",
        "        # fh.setLevel(logging.DEBUG)\n",
        "        fh.setFormatter(formatter)\n",
        "        logger.addHandler(fh)\n",
        "\n",
        "        ch = logging.StreamHandler(sys.stdout)\n",
        "        # ch.setLevel(level)\n",
        "        ch.setFormatter(formatter)\n",
        "        logger.addHandler(ch)\n",
        "        \n",
        "        loggers[name] = logger\n",
        "        return logger\n",
        "\n",
        "logger = set_logger('sa_tweet_inperf', logging.DEBUG)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuPE0FY7Nasq",
        "outputId": "0dd9c6f3-8df8-4475-cec6-a92ca99f0f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Import Common modules\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Data Science modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "# Set Seaborn Style\n",
        "sns.set(style='white', context='notebook', palette='deep')\n",
        "\n",
        "# Check GPU Device\n",
        "import torch\n",
        "if IN_COLAB:\n",
        "    logger.info(\"GPU Device: {}\".format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-21 13:12:55,183][INFO] ## GPU Device: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8ydqOKGw5pP",
        "scrolled": true,
        "outputId": "caffe261-005b-477a-f320-537423d69caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "# Note: After the following pip install, you may need to restart the kernel.\n",
        "# Required modules - written here so that it can setup a new instance in Colab. Not required to run every time.\n",
        "is_first = False\n",
        "\n",
        "if IN_COLAB or is_first:\n",
        "    !pip install scikit-plot transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.6/dist-packages (0.3.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYiY1suXzjKg"
      },
      "source": [
        "# # Python libraries\n",
        "# import datetime as dt\n",
        "# import random\n",
        "\n",
        "\n",
        "# # Import Scikit-learn moduels\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn import model_selection\n",
        "# from sklearn.model_selection import StratifiedKFold, learning_curve, StratifiedShuffleSplit\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYf9oi6fw5pV"
      },
      "source": [
        "# Set Random Seed\n",
        "# random.seed(42)\n",
        "# np.random.seed(42)\n",
        "rand_seed = 42\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLFBUS8Hw5pf"
      },
      "source": [
        "# Specify file locations\n",
        "if IN_COLAB:\n",
        "    tweet_dir = 'drive/My Drive/Colab Data/'\n",
        "    output_dir = 'drive/My Drive/Colab Data/'\n",
        "else:\n",
        "    tweet_dir = './data/tweet/'\n",
        "    output_dir = './data/result/'\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy1s4WxDDEnm"
      },
      "source": [
        "## Evaluation Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKI-PTvA3PFH"
      },
      "source": [
        "### Metrics\n",
        "As the data are imbalanced, use F1 score (micro) in addition to Accuracy. For cross validation, refit to F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEax7Gmow5rJ"
      },
      "source": [
        "# Define metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import scikitplot as skplt\n",
        "\n",
        "# Here, use F1 Macro to evaluate the model.\n",
        "def metric(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "scoring = {'Accuracy': 'accuracy', 'F1': 'f1_macro'}\n",
        "refit = 'F1'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D24v2gPBmoC"
      },
      "source": [
        "## Load Input Data\n",
        "Tweet data from Stocktwit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z4fHFfOBqVs"
      },
      "source": [
        "import json\n",
        "\n",
        "def load_tweet(filename):\n",
        "    ''' Clean FinancialPhrasebank data\n",
        "        Input:\n",
        "            - filename\n",
        "        Output:\n",
        "            - a dataframe for the loaded financial phase bank data\n",
        "    '''\n",
        "    with open(tweet_dir + filename, 'r') as f:\n",
        "        twits = json.load(f)\n",
        "\n",
        "    logger.debug(twits['data'][:10])\n",
        "    logger.info(\"The number of twits is: {}\".format(len(twits['data'])))\n",
        "    messages = [twit['message_body'] for twit in twits['data']]\n",
        "    # Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in the network\n",
        "    sentiments = [twit['sentiment'] + 2 for twit in twits['data']]\n",
        "    \n",
        "    return messages, sentiments"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zANMDMcWZWZC",
        "outputId": "6c38b855-a6f5-4d35-e6ed-a2722208eb6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Load data\n",
        "filename = 'twits.json'\n",
        "messages, sentiments = load_tweet(filename)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-21 13:12:59,911][DEBUG] ## [{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard thereâ€™s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n",
            "[2020-10-21 13:12:59,912][INFO] ## The number of twits is: 1548010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3qFF261ZpU0"
      },
      "source": [
        "## Process Input text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94Kf7SRaZuCk"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(message):\n",
        "    \"\"\"\n",
        "    This function takes a string as input, then performs these operations: \n",
        "        - lowercase\n",
        "        - remove URLs\n",
        "        - remove ticker symbols \n",
        "        - removes punctuation\n",
        "        - tokenize by splitting the string on whitespace \n",
        "        - removes any single character tokens\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        message : The text message to be preprocessed.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        tokens: The preprocessed text into tokens.\n",
        "    \"\"\" \n",
        "    # Lowercase the twit message\n",
        "    text = message.lower()\n",
        "    \n",
        "    # Replace URLs with a space in the message\n",
        "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
        "    \n",
        "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
        "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
        "    \n",
        "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
        "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
        "\n",
        "    # Replace everything not a letter or apostrophe with a space\n",
        "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
        "\n",
        "    # Remove single letter words\n",
        "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tTibTGZZw8w",
        "outputId": "1359761d-7573-421d-d7c0-f57f975c61eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Check\n",
        "test_message = \"RT @google Our annual looked at the year in a Google's blogging (and beyond) http://t.co/sptHOAh8 $GOOG\"\n",
        "print(preprocess(test_message))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rt our annual looked at the year in google's blogging and beyond\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiCXF6fYZ0Sy",
        "outputId": "2e0efddc-0ac3-4829-f369-7857db43e85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "311d0ea8f9db4db9bb17f89182fd4abf",
            "89db5b7536df4d82ac976fccb1b11043",
            "2a88a8f837124f3892888d1138e60ff0",
            "fef32952b6d44e8f8de86e69fca18c3d",
            "5c51e4d4ac5b42e6bce3e71c33159ca4",
            "1a8da457b7444ba38215c12610f43606",
            "a3da80801864456c94ce2a20fd620b15",
            "6ff0d18991074416baa9d2dc225214b8"
          ]
        }
      },
      "source": [
        "# Process for all messages\n",
        "preprocessed = [preprocess(message) for message in tqdm(messages)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "311d0ea8f9db4db9bb17f89182fd4abf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1548010.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeXaddnbw5p9"
      },
      "source": [
        "## Explore the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202rvKHUPyUY"
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def tokenize_text(text, option):\n",
        "    '''\n",
        "    Tokenize the input text as per specified option\n",
        "      1: Use python split() function\n",
        "      2: Use regex to extract alphabets plus 's and 't\n",
        "      3: Use NLTK word_tokenize()\n",
        "      4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
        "    '''\n",
        "    if option == 1:\n",
        "        return text.split()\n",
        "    elif option == 2:\n",
        "        return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
        "    elif option == 3:\n",
        "        return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
        "    elif option == 4:\n",
        "        words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
        "        # Remove stop words\n",
        "        stop = set(stopwords.words('english'))\n",
        "        words = [word for word in words if (word not in stop)]\n",
        "        # Lemmatize words (first noun, then verb)\n",
        "        wnl = nltk.stem.WordNetLemmatizer()\n",
        "        lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
        "        return lemmatized\n",
        "    else:\n",
        "        logger.warn(\"Please specify option value between 1 and 4\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azbryaBq3ALp"
      },
      "source": [
        "tokenize_text(preprocessed[0], 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr8wjmuS2_aA"
      },
      "source": [
        "# Create vocab\n",
        "def create_vocab(messages, show_graph=False):\n",
        "    corpus = []\n",
        "    for message in tqdm(messages, desc=\"Tokenizaing\"):\n",
        "        tokens = tokenize_text(message, 3) # Use option 3\n",
        "        corpus.extend(tokens)\n",
        "    logger.info(\"The number of all words: {}\".format(len(corpus)))\n",
        "\n",
        "    # Create Counter\n",
        "    counts = Counter(corpus)\n",
        "    logger.info(\"The number of unique words: {}\".format(len(counts)))\n",
        "\n",
        "    # Create BoW\n",
        "    bow = sorted(counts, key=counts.get, reverse=True)\n",
        "    logger.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
        "\n",
        "    # Indexing vocabrary, starting from 1.\n",
        "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
        "    id2vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    if show_graph:\n",
        "        from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "        # Generate Word Cloud image\n",
        "        text = \" \".join(corpus)\n",
        "        stopwords = set(STOPWORDS)\n",
        "        stopwords.update([\"will\", \"report\", \"reporting\", \"market\", \"stock\", \"share\"])\n",
        "\n",
        "        wordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(text)\n",
        "        plt.figure(figsize=(15,7))\n",
        "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        # Show most frequent words in a bar graph\n",
        "        most = counts.most_common()[:80]\n",
        "        x, y = [], []\n",
        "        for word, count in most:\n",
        "            if word not in stopwords:\n",
        "                x.append(word)\n",
        "                y.append(count)\n",
        "        plt.figure(figsize=(12,10))\n",
        "        sns.barplot(x=y, y=x)\n",
        "        plt.show()\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab= create_vocab(preprocessed, True)\n",
        "\n",
        "## Create token id list\n",
        "#token_ids = [[vocab[word] for word in text_words] for text_words in tokenized] # comment out to save memory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNxrQ5KQaaFj"
      },
      "source": [
        "tmp_dict = {'org message': messages, 'sentence': preprocessed, 'label': sentiments}\n",
        "tmp_df = pd.DataFrame(tmp_dict).sample(n=20, random_state=rand_seed)\n",
        "\n",
        "# Samples\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "tmp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHi-4D5Iad_D"
      },
      "source": [
        "# Change the table display config back\n",
        "pd.set_option('display.max_colwidth', 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31UOZV7gavDC"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc_K8LliPRzt"
      },
      "source": [
        "# Create a dataframe for training data\n",
        "word_cnt = [len(tokenize_text(x, 3)) for x in tqdm(preprocessed)]\n",
        "\n",
        "# Use tweets having 5 or more words. Do not resample for balancing data here.\n",
        "train_dict = {'text': preprocessed, 'label': sentiments, 'count': word_cnt}\n",
        "train_df = pd.DataFrame(train_dict)\n",
        "train_df = train_df.loc[train_df['count'] >= 5]\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "logger.info(\"The total number of input data: {}\".format(len(train_df)))\n",
        "\n",
        "# Display the distribution graph\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(17,5))\n",
        "sns.countplot(x='label', data=train_df, ax=ax1)\n",
        "ax1.set_title('The number of data for each label', fontsize=14)\n",
        "sns.histplot([len(x) for x in train_df['text']], ax=ax2, bins=100)\n",
        "ax2.set_title('The number of letters in each data', fontsize=14)\n",
        "ax2.set_xlim(0,150)\n",
        "ax2.set_xlabel('number of letters')\n",
        "ax2.set_ylabel(\"\")\n",
        "sns.histplot(train_df['count'], ax=ax3, bins=100)\n",
        "ax3.set_title('The number of words in each data', fontsize=14)\n",
        "ax3.set_xlim(0,40)\n",
        "ax3.set_xlabel('number of words')\n",
        "ax3.set_ylabel(\"\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jys8ypOs_JV-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpeCGbeO_sqw"
      },
      "source": [
        "# Define Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9RTS-OLLod"
      },
      "source": [
        "# Import Pytorch modules\n",
        "# import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from transformers import AdamW as AdamW_HF, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDozi5NLAHn9"
      },
      "source": [
        "## Dataset and loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S9wWGsMFIXD"
      },
      "source": [
        "# Define a DataSet Class which simply return (x, y) pair\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
        "    def __len__(self):\n",
        "        return len(self.datalist)\n",
        "    def __getitem__(self,idx):\n",
        "        return self.datalist[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKQPzVaSAjag"
      },
      "source": [
        "# Data Loader\n",
        "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
        "    X_sampled = np.array(X, dtype=object)[indices]\n",
        "    y_sampled = np.array(y)[indices].astype(int)\n",
        "    dataset = SimpleDataset(X_sampled, y_sampled)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKjNhQ55AWv2"
      },
      "source": [
        "## Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoxhMegUU-Tg"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
        "    result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
        "\n",
        "    for n in num_samples:\n",
        "        print(\"\")\n",
        "        logger.info(\"############### Start training for %d samples ###############\" %n)\n",
        "\n",
        "        # Stratified sampling\n",
        "        train_size = n / len(y_all)\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2 , random_state=rand_seed)\n",
        "        train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
        "\n",
        "        # Sample input data\n",
        "        train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
        "        valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
        "\n",
        "        if model_type == 'LSTM':\n",
        "            model = TextClassifier(len(vocab)+1, embed_size=512, lstm_size=1289, dense_size=0, output_size=5, lstm_layers=4, dropout=0.2)\n",
        "            model.embedding.weight.data.uniform_(-1, 1)\n",
        "        elif model_type == 'BERT':\n",
        "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "\n",
        "        start_time = time.perf_counter() # use time.process_time() for CPU time\n",
        "        acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
        "        end_time = time.perf_counter() # use time.process_time() for CPU time\n",
        "        duration = end_time - start_time\n",
        "        logger.info(\"Process Time (sec): {}\".format(duration))\n",
        "        result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
        "\n",
        "    return result, model_trained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zVsHeumUMmP"
      },
      "source": [
        "def train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
        "    # Set variables\n",
        "    logger = set_logger('sa_tweet_inperf', log_level)\n",
        "    num_total_opt_steps = int(len(train_loader) * epochs)\n",
        "    eval_every = len(train_loader) // 5\n",
        "    warm_up_proportion = 0.1\n",
        "    logger.info('Total Training Steps: {} ({} batches x {} epochs)'.format(num_total_opt_steps, len(train_loader), epochs))\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW_HF(model.parameters(), lr=lr, correct_bias=False) \n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_total_opt_steps*warm_up_proportion, num_training_steps=num_total_opt_steps)  # PyTorch scheduler\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Set Train Mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialise\n",
        "    acc_train, f1_train, loss_train, acc_valid, f1_valid, loss_valid = [], [], [], [], [], []\n",
        "    best_f1, early_stop, steps = 0, 0, 0\n",
        "    class_names = ['0:Very Negative','1:Negative', '2:Neutral', '3:Positive', '4:Very Positive']\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "        logger.info('================     epoch {}     ==============='.format(epoch+1))\n",
        "\n",
        "        #################### Training ####################\n",
        "        # Initialise\n",
        "        loss_tmp, loss_cnt = 0, 0\n",
        "        y_pred_tmp, y_truth_tmp = [], []\n",
        "        hidden = model.init_hidden(batch_size) if model_type == \"LSTM\" else None\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            text_batch, labels = batch\n",
        "            # Skip the last batch of which size is not equal to batch_size\n",
        "            if labels.size(0) != batch_size:\n",
        "                break\n",
        "            steps += 1\n",
        "           \n",
        "            # Reset gradient\n",
        "            model.zero_grad()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Initialise after the previous training\n",
        "            if steps % eval_every == 1:\n",
        "                y_pred_tmp, y_truth_tmp = [], []\n",
        "\n",
        "            if model_type == \"LSTM\":\n",
        "                # Tokenize the input and move to device\n",
        "                text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
        "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "\n",
        "                # Creating new variables for the hidden state to avoid backprop entire training history\n",
        "                hidden = tuple([each.data for each in hidden])\n",
        "                for each in hidden:\n",
        "                    each.to(device)\n",
        "\n",
        "                # Get output and hidden state from the model, calculate the loss\n",
        "                logits, hidden = model(text_batch, hidden)\n",
        "                loss = criterion(logits, labels)\n",
        "                \n",
        "            elif model_type == 'BERT':\n",
        "                # Tokenize the input and move to device\n",
        "                # Tokenizer Parameter\n",
        "                param_tk = {\n",
        "                    'return_tensors': \"pt\",\n",
        "                    'padding': 'max_length',\n",
        "                    'max_length': seq_len,\n",
        "                    'add_special_tokens': True,\n",
        "                    'truncation': True\n",
        "                }\n",
        "                text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
        "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "\n",
        "                # Feedforward prediction\n",
        "                loss, logits = model(**text_batch, labels=labels)\n",
        "\n",
        "            y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
        "            y_truth_tmp.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Back prop\n",
        "            loss.backward()\n",
        "\n",
        "            # Training Loss\n",
        "            loss_tmp += loss.item()\n",
        "            loss_cnt += 1\n",
        "\n",
        "            # Clip the gradient to prevent the exploading gradient problem in RNN/LSTM\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            # Update Weights and Learning Rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "            #################### Evaluation ####################\n",
        "            if (steps % eval_every == 0) or ((steps % eval_every != 0) and (steps == len(train_loader))):\n",
        "                # Evaluate Training\n",
        "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
        "                acc_train.append(acc)\n",
        "                f1_train.append(f1)\n",
        "                loss_train.append(loss_tmp/loss_cnt)\n",
        "                loss_tmp, loss_cnt = 0, 0\n",
        "\n",
        "                # y_pred_tmp = np.zeros((len(y_valid), 5))\n",
        "                y_truth_tmp, y_pred_tmp = [], []\n",
        "\n",
        "                # Move to Evaluation Mode\n",
        "                model.eval()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for i, batch in enumerate(valid_loader):\n",
        "                        text_batch, labels = batch\n",
        "                        # Skip the last batch of which size is not equal to batch_size\n",
        "                        if labels.size(0) != batch_size:\n",
        "                            break\n",
        "\n",
        "                        if model_type == \"LSTM\":\n",
        "                            # Tokenize the input and move to device\n",
        "                            text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
        "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "\n",
        "                            # Creating new variables for the hidden state to avoid backprop entire training history\n",
        "                            hidden = tuple([each.data for each in hidden])\n",
        "                            for each in hidden:\n",
        "                                each.to(device)\n",
        "\n",
        "                            # Get output and hidden state from the model, calculate the loss\n",
        "                            logits, hidden = model(text_batch, hidden)\n",
        "                            loss = criterion(logits, labels)\n",
        "                \n",
        "                        elif model_type == 'BERT':\n",
        "                            # Tokenize the input and move to device\n",
        "                            text_batch = tokenizer_bert(text_batch, **param_tk).to(device)\n",
        "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
        "                            # Feedforward prediction\n",
        "                            loss, logits = model(**text_batch, labels=labels)\n",
        "                    \n",
        "                        loss_tmp += loss.item()\n",
        "                        loss_cnt += 1\n",
        "\n",
        "                        y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
        "                        y_truth_tmp.extend(labels.cpu().numpy())\n",
        "                        # logger.debug('validation batch: {}, val_loss: {}'.format(i, loss.item() / len(valid_loader)))\n",
        "\n",
        "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
        "                logger.debug(\"Epoch: {}/{}, Step: {}, Loss: {:.4f}, Acc: {:.4f}, F1: {:.4f}\".format(epoch+1, epochs, steps, loss_tmp, acc, f1))\n",
        "                acc_valid.append(acc)\n",
        "                f1_valid.append(f1)\n",
        "                loss_valid.append(loss_tmp/loss_cnt)\n",
        "                loss_tmp, loss_cnt = 0, 0\n",
        "\n",
        "                # Back to train mode\n",
        "                model.train()\n",
        "\n",
        "        #################### End of each epoch ####################\n",
        "\n",
        "        # Show the last evaluation metrics\n",
        "        logger.info('Epoch: %d, Loss: %.4f, Acc: %.4f, F1: %.4f, LR: %.2e' % (epoch+1, loss_valid[-1], acc_valid[-1], f1_valid[-1], scheduler.get_last_lr()[0]))\n",
        "\n",
        "        # Plot Confusion Matrix\n",
        "        y_truth_class = [class_names[int(idx)] for idx in y_truth_tmp]\n",
        "        y_predicted_class = [class_names[int(idx)] for idx in y_pred_tmp]\n",
        "        \n",
        "        titles_options = [(\"Actual Count\", None), (\"Normalised\", 'true')]\n",
        "        for title, normalize in titles_options:\n",
        "            disp = skplt.metrics.plot_confusion_matrix(y_truth_class, y_predicted_class, normalize=normalize, title=title, x_tick_rotation=75)\n",
        "        plt.show()\n",
        "\n",
        "        # plot training performance\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
        "        ax1.set_title(\"Losses\")\n",
        "        ax1.set_xlabel(\"Validation Cycle\")\n",
        "        ax1.set_ylabel(\"Loss\")\n",
        "        ax1.plot(loss_train, 'b-o', label='Train Loss')\n",
        "        ax1.plot(loss_valid, 'r-o', label='Valid Loss')\n",
        "        ax1.legend(loc=\"upper right\")\n",
        "        \n",
        "        ax2.set_title(\"Evaluation\")\n",
        "        ax2.set_xlabel(\"Validation Cycle\")\n",
        "        ax2.set_ylabel(\"Score\")\n",
        "        ax2.set_ylim(0,1)\n",
        "        ax2.plot(acc_train, 'y-o', label='Accuracy (train)')\n",
        "        ax2.plot(f1_train, 'y--', label='F1 Score (train)')\n",
        "        ax2.plot(acc_valid, 'g-o', label='Accuracy (valid)')\n",
        "        ax2.plot(f1_valid, 'g--', label='F1 Score (valid)')\n",
        "        ax2.legend(loc=\"upper left\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # If improving, save the number. If not, count up for early stopping\n",
        "        if best_f1 < f1_valid[-1]:\n",
        "            early_stop = 0\n",
        "            best_f1 = f1_valid[-1]\n",
        "        else:\n",
        "            early_stop += 1\n",
        "\n",
        "        # Early stop if it reaches patience number\n",
        "        if early_stop >= patience:\n",
        "            break\n",
        "\n",
        "        # Prepare for the next epoch\n",
        "        if device == 'cuda:0':\n",
        "            torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "\n",
        "    return acc, f1, model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUkUbGsw5rX"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKwTVfCcw5ra"
      },
      "source": [
        "# Define LSTM Model\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.dense_size = dense_size\n",
        "        self.output_size = output_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if dense_size == 0:\n",
        "            self.fc = nn.Linear(lstm_size, output_size)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(lstm_size, dense_size)\n",
        "            self.fc2 = nn.Linear(dense_size, output_size)\n",
        "\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Initialize the hidden state\n",
        "        \"\"\"\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
        "                  weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
        "                \n",
        "        return hidden\n",
        "\n",
        "    def forward(self, nn_input_text, hidden_state):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of the model on nn_input\n",
        "        \"\"\"\n",
        "        batch_size = nn_input_text.size(0)\n",
        "        nn_input_text = nn_input_text.long()\n",
        "        embeds = self.embedding(nn_input_text)\n",
        "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
        "        # Stack up LSTM outputs, apply dropout\n",
        "        lstm_out = lstm_out[-1,:,:]\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        # Dense layer\n",
        "        if self.dense_size == 0:\n",
        "            out = self.fc(lstm_out)\n",
        "        else:\n",
        "            dense_out = self.fc1(lstm_out)\n",
        "            out = self.fc2(dense_out)\n",
        "        # Softmax\n",
        "        logps = self.softmax(out)\n",
        "\n",
        "        return logps, hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_-Spjy5A1hv"
      },
      "source": [
        "# Define a tokenizer\n",
        "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
        "    X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
        "    for i, text in enumerate(X):\n",
        "        tokens = tokenize_text(text, 3) \n",
        "        token_ids = [vocab[word] for word in tokens]\n",
        "        end_idx = min(len(token_ids), seq_len)\n",
        "        if padding == 'right':\n",
        "            X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
        "        elif padding == 'left':\n",
        "            start_idx = max(seq_len - len(token_ids), 0)\n",
        "            X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
        "\n",
        "    return torch.tensor(X_tmp, dtype=torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adhXylejw5rf"
      },
      "source": [
        "### Configure the model and train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u-uab24U1wE"
      },
      "source": [
        "# Define the training parameters\n",
        "num_samples = [1000, 5000, 10000, 100000, 500000]\n",
        "epochs=5\n",
        "patience=3\n",
        "batch_size=64\n",
        "seq_len = 30\n",
        "lr=3e-4\n",
        "clip=5\n",
        "log_level=logging.DEBUG\n",
        "\n",
        "# Run!\n",
        "result_lstm, model_trained_lstm = train_cycles(train_df['text'], train_df['label'], vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
        "result_lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fd44Xjhw5r9"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8wGfwJRAKsU"
      },
      "source": [
        "# Use pretrained model\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "\n",
        "# Tokenizer Parameter\n",
        "param_tk = {\n",
        "    'return_tensors': \"pt\",\n",
        "    'padding': 'max_length',\n",
        "    'max_length': seq_len,\n",
        "    'add_special_tokens': True,\n",
        "    'truncation': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdZf51Gw5sE",
        "scrolled": true
      },
      "source": [
        "# Test the model and tokenizer\n",
        "inputs = tokenizer_bert(\"Hello, my dog is cute\", **param_tk)\n",
        "\n",
        "print('inputs: \\n', inputs)\n",
        "print('\\ndecoded: \\n',tokenizer_bert.decode(inputs['input_ids'].squeeze(0)))\n",
        "\n",
        "labels = torch.tensor([1]).unsqueeze(0)\n",
        "print('\\nlabels: ', labels)\n",
        "\n",
        "outputs = bert_model(**inputs, labels=labels)\n",
        "print('\\noutputs: length=', len(outputs))\n",
        "print(outputs)\n",
        "\n",
        "loss = outputs[0]\n",
        "logits = outputs[1]\n",
        "\n",
        "print('loss: ', loss.detach())\n",
        "print('logits: ', logits.detach())\n",
        "\n",
        "print(len(outputs))\n",
        "print('outputs: \\n',outputs)\n",
        "print('outputs(detached): \\n', outputs[0].detach())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JskUht06X1oE"
      },
      "source": [
        "### Configure the model and train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peYs1C_7w5sF"
      },
      "source": [
        "# Define the training parameters\n",
        "num_samples = [1000, 5000, 10000, 100000, 500000]\n",
        "epochs=5\n",
        "patience=3\n",
        "batch_size=64\n",
        "seq_len = 30\n",
        "lr=2e-5\n",
        "clip=1.0\n",
        "log_level=logging.DEBUG\n",
        "\n",
        "# Run!\n",
        "result_bert, model_trained_bert = train_cycles(train_df['text'], train_df['label'], vocab, num_samples, 'BERT', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
        "result_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdEn3xMNw5sI"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqwUOnPtw5sJ"
      },
      "source": [
        "# If disconnected after the training complete, just recreate the result_df rather than running the training again...\n",
        "disconnected = False\n",
        "if disconnected:\n",
        "    n_trains = [1000, 5000, 10000, 100000, 500000]\n",
        "    result_lstm = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time'], index=n_trains)\n",
        "    result_lstm.loc[1000] = (0.4115,\t0.1166,\t4.02131)\n",
        "    result_lstm.loc[5000] = (0.4969,\t0.4542,\t13.189)\n",
        "    result_lstm.loc[10000] = (0.5514,\t0.4988,\t23.8845)\n",
        "    result_lstm.loc[100000] = (0.6856,\t0.6548,\t273.114)\n",
        "    result_lstm.loc[500000] = (0.7593,\t0.7375,\t2704.45)\n",
        "\n",
        "result_lstm['ms/data'] = result_lstm['Total_Time'] / result_lstm.index * 1000\n",
        "result_lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7KulavgCcV1"
      },
      "source": [
        "# If disconnected after the training complete, just recreate the result_df rather than running the training again...\n",
        "disconnected = False\n",
        "if disconnected:\n",
        "    n_trains = [1000, 5000, 10000, 100000, 500000]\n",
        "    result_bert = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time'], index=n_trains)\n",
        "    result_bert.loc[1000] = (0.465,\t0.2876,\t25.8849)\n",
        "    result_bert.loc[5000] = (0.5355,\t0.4475,\t56.1339)\n",
        "    result_bert.loc[10000] = (0.6482,\t0.6039,\t110.48)\n",
        "    result_bert.loc[100000] = (0.7453,\t0.7223,\t1092.68)\n",
        "    result_bert.loc[500000] = (0.7889,\t0.7693,\t5443.28)\n",
        "\n",
        "result_bert['ms/data'] = result_bert['Total_Time'] / result_bert.index * 1000\n",
        "result_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76A5wgzebaQI"
      },
      "source": [
        "# Extra - tweet stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSawHYO3w5sM"
      },
      "source": [
        "with open(tweet_dir + 'test_twits.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "def twit_stream():\n",
        "    for twit in test_data['data']:\n",
        "        yield twit\n",
        "\n",
        "next(twit_stream())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeWHc7RXbpzd"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(text, model, tokenizer):\n",
        "    \"\"\" \n",
        "    Make a prediction on a single sentence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        text : The string to make a prediction on.\n",
        "        model : The model to use for making the prediction.\n",
        "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        pred : Prediction vector\n",
        "    \"\"\"   \n",
        "    text = preprocess(text)\n",
        "    inputs = tokenizer(text, \n",
        "                   return_tensors=\"pt\", \n",
        "                   padding='max_length',\n",
        "                   max_length=96,\n",
        "                   add_special_tokens=True,\n",
        "                   truncation=True)\n",
        "\n",
        "    outputs = model(**inputs)[0].detach()    \n",
        "    pred = F.softmax(outputs, dim=1)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dAPQYCubxQD"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model.load_state_dict(torch.load(output_dir + 'stocktwit_bert.dict'))\n",
        "model.eval()\n",
        "model.to(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuOD7Efkbz2y"
      },
      "source": [
        "# Check\n",
        "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
        "predict(text, model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2QsRrnab3Qx"
      },
      "source": [
        "def score_twits(stream, model, tokenizer, universe):\n",
        "    \"\"\" \n",
        "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
        "    \"\"\"\n",
        "    class_names = ['0:Very Negative', '1:Negative', '2:Neutral', '3:Positive', '4:Very Positive']\n",
        "    for twit in stream:\n",
        "\n",
        "        # Get the message text\n",
        "        text = twit['message_body']\n",
        "        if len(tokenizer.tokenize(preprocess(text))) < 10:\n",
        "            continue\n",
        "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
        "        score = predict(text, model, tokenizer)\n",
        "        score = np.round(score.tolist(), 4).squeeze()\n",
        "        prediction = class_names[np.argmax(score)] + \" {:.1f}%\".format(np.max(score)*100)\n",
        "\n",
        "        for symbol in symbols:\n",
        "            if symbol in universe:\n",
        "                yield {'symbol': symbol, 'pred': prediction, 'score': score, 'text': text, 'timestamp': twit['timestamp']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ocBVcJb4Do"
      },
      "source": [
        "# Select Universe\n",
        "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
        "score_stream = score_twits(twit_stream(), model, tokenizer, universe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO6HDaIgb8rK"
      },
      "source": [
        "# Process\n",
        "for i in range(10):\n",
        "    print(next(score_stream))\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JTQaPSSHPRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}